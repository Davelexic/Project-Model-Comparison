{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution CNN for leaning images and processing - using high-level Keras preprocessing utilities and layers to read a directory of images on disk.\n",
    "\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "candlesticks/<br />\n",
    "    buy<br />\n",
    "    sell<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define path for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path('candlesticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate total candlestick counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read a sample buy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAMBElEQVR4nO3dQWpc2RmG4d9uRaiJBqIJVnbgqbeQmSFLSFbQm2i8id5BlhDwLFvwtHcQQQY1ELRRutsZqEBxwLYgrnNL9T4P3PF3zsD4RdKtejYzHwYAgIznWx8AAIC1BCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAfebP1AQCAgxOAfOTV1gcAAA5OAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAJ/wZusDAByIAAT4hFdbHwDgQAQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMWdbHwDgMa5n5mrx5uXMvFy8uZuZm8WbQI8ABI7e9cy8nfUB+GK/u9JuZl6PCAQOSwACR+9q/9ztn1W+m5nbhXvn83BXAQgckgAEnoy7mXm/cO+3xXsz9xEIcGheAgEAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADFnWx8A4LHOF+89n5mLhXur7wd0CUDg6O32z9WsjaSzmblcuDfzcFeAQxKAwNG7mZnXcx+AK/04M98v3tzN/X0BDkkAAk/CzawPo9uZ+WnxJsAKXgIBAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAE4am+2PgCcIAEIwFF7tfUB4AQJQACAGAEIABAjAAEAYgQgAECMAAQAiDnb+gB82vXMXC3evJyZl4s3dzNzs3gTAMoE4JG6npm3sz4AX+x3V9rNzOsRgQCwigA8Ulf7527/rPLdzNwu3Dufh7sKwP/yp5n5x9aHAOBUCcAjdzcz7xfu/bZ4b+Y+Avkff9z6AACcMi+BAADECEAAgBgBCAAQIwABAGIEIADwRW+2PgBflQAEAL7o1dYH4KsSgAAAMQIQACDGB0HDl1zOzMXizfOZ+cPizfez9mtgANiMAITPuZyZv8zMt4t3fz8zf128+fPM/G1EIECAAITPuZj7+Pt1/6zy7az9Euhv9psXIwABAgQgPMavM/PLwr0Pi/dm7iMQeBKuZ+Zq8eblzLxcvLmbmZvFmxUCEACekOuZeTvrA/DFfnel3cy8HhF4CAIQAJ6Qq/1zN2v/UuS7WfsXIufzcFcB+PUJQAB4gu7m/uX9VX5bvDdzH4Echs8BBACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxJxtfQAAno7rmblavHk5My8Xb+5m5mbxJqwkAAF4lOuZeTvrA/DFfnel3cy8HhHI6RKAADzK1f55NjP/Xrj7YWZ+Wbj3u3m4qwDkVAlAAB5lt3+uZu1/Hs8W732Yh7vCqRKAADzKzdz/WvRq8e6PM/P94s3d+Okfp00AAnzCu60PcIRuZn0Y3c7MT4s34dT5GBiAT/hh6wMAHIgABACIEYAAADECEAAgRgACAMQIQACAGAEIABDjcwAB4Ak6X7z3fGYuFu6tvl+NAASAJ2Q3D1/JtzKSzmbmcuHejK/kOyQBCABPiK/k42sQgADwxPhKPv5fXgIBAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAjtq7rQ8AJ0gAAnDUftj6AHCCBCAAQIwABACIEYAAwBe92/oAfFUCEAD4In+LeVoEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAEDM2dYH4PPOF+89n5mLhXur7wcACMCjtds/V7M2ks5m5nLh3szDXQGANQTgkbqZmddzH4Ar/Tgz3y/e3M39fQGANQTgEbuZ9WF0OzM/Ld4EANbyEggAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAOQj77Y+AABwcAKQj/yw9QEAgIMTgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgJizrQ8AT8I3i/eezdp/navvB8CmBCB8zvuZ+Xlmvp21kfR8Zs4X7s3c3/P94k0ANvFsZj5sfQg4apczc7F4888z8/fFm+9n5nbxJgCb8BNA+JLbWR9GdzPzr8WbAGR4CQQAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEA4Rv/c+gAAnLJnM/Nh60MAALCOnwACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBi/gMiDI16r+ttjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=640x480 at 0x1B3EB7677C8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candlesticks = list(data_dir.glob('buy/*.png'))\n",
    "PIL.Image.open(str(candlesticks[0]))\n",
    "#PIL.Image.open(str(candlesticks[0])).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load using tf.keras.preprocessing.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset\n",
    "\n",
    "##### Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 1\n",
    "img_height = 255\n",
    "img_width = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's good practice to use a validation split when developing your model. Will use 80% of the images for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 files belonging to 2 classes.\n",
      "Using 43 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width)\n",
    "#  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 files belonging to 2 classes.\n",
      "Using 10 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width)\n",
    "  #batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can find the class names in the class_names attribute on these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'sell']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the first 9 images from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we wil train a model using these datasets by passing them to model.fit. You can also manually iterate over the dataset and retrieve batches of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 255, 255, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The image_batch is a tensor of the shape (1, 180, 180, 3). This is a batch of 1 images of shape 180x180x3 (the last dimension referes to color channels RGB). The label_batch is a tensor of the shape (1,), these are the corresponding labels to the 32 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, you will standardize values to be in the [0, 1] range by using the tf.keras.layers.experimental.preprocessing.Rescaling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are two ways to use this layer. You can apply it to the dataset by calling map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.87404835\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Or, you can include the layer inside your model definition to simplify deployment. We will use the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure to use buffered prefetching, so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    ".cache() keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    ".prefetch() overlaps data preprocessing and model execution while training.\n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the data performance guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, you will show how to train a simple model using the datasets you have just prepared. This model has not been tuned in any way - the goal is to show you the mechanics using the datasets you just created. To learn more about image classification, visit this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "#  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 images belonging to 2 classes.\n",
      "Found 53 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=False)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "            '/content/drive/My Drive/Colab Notebooks/dataset',\n",
    "            target_size=(128, 128),\n",
    "            batch_size=30,\n",
    "            class_mode='categorical')\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory(\n",
    "#             '../dataset/test_set/',\n",
    "#             target_size=(64, 64),\n",
    "#             batch_size=32,\n",
    "#             class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "            training_set,\n",
    "            steps_per_epoch=40,\n",
    "            epochs=10,\n",
    "            validation_data=training_set,\n",
    "            validation_steps=80)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy', #tf.losses.categorical_crossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b385bf2048>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_set.next()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2589 - accuracy: 0.9375\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 0.1279 - accuracy: 0.9688\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 0.1854 - accuracy: 0.9062\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1527 - accuracy: 0.9375\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 1s 899ms/step - loss: 0.1301 - accuracy: 0.9524\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 0.0856 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 0.0997 - accuracy: 0.9688\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 0.0747 - accuracy: 0.9688\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 1s 976ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 1s 807ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0538 - accuracy: 0.9688\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.0883 - accuracy: 0.9524\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 1s 786ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0647 - accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b38771a4c8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            training_set, \n",
    "            steps_per_epoch=1,\n",
    "            epochs=20,\n",
    "#            validation_split=.1\n",
    "#            validation_data=validation_set\n",
    "#            validation_data=training_set,\n",
    "#            validation_steps=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 5.6635 - val_accuracy: 0.6000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 6.5766 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 4s 1s/step - loss: 5.9096e-04 - accuracy: 1.0000 - val_loss: 7.4365 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 4s 885ms/step - loss: 2.1023e-04 - accuracy: 1.0000 - val_loss: 8.2343 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 3s 965ms/step - loss: 8.2910e-05 - accuracy: 1.0000 - val_loss: 8.9625 - val_accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 4s 963ms/step - loss: 3.8105e-05 - accuracy: 1.0000 - val_loss: 9.6195 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 20s 1s/step - loss: 2.0060e-05 - accuracy: 1.0000 - val_loss: 10.2037 - val_accuracy: 0.6000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 6s 2s/step - loss: 1.1593e-05 - accuracy: 1.0000 - val_loss: 10.7145 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 7s 1s/step - loss: 7.0578e-06 - accuracy: 1.0000 - val_loss: 11.1577 - val_accuracy: 0.6000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 7s 1s/step - loss: 4.4715e-06 - accuracy: 1.0000 - val_loss: 11.5409 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b381f3a2c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10,\n",
    "#  verbose='2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 269ms/step - loss: 0.0053 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005253512412309647, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepenv] *",
   "language": "python",
   "name": "conda-env-deepenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
